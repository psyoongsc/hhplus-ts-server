# STEP20 (가상) 장애 대응 보고서

## 1. 장애 상황 식별

- **식별 수단**: Slack Alert (API 응답 지연 - 평균 200ms 이상)
- **발생 일시**: 2025년 6월 5일 22:35 ~ 23:12
- **영향 범위**: 전체 API 응답 지연
- **정상 응답 시간**: 평균 20ms
- **장애 발생 후 응답 시간**: 평균 281.76ms, 최대 510.74ms
- **요약**: API 서버에서 병목 현상이 발생하여 전반적인 응답 시간이 급증함. 사용자 요청이 1 RPS 수준으로 낮았음에도 응답 지연이 관측됨.

### 📊 관측 지표

| 지표 항목 | 측정값 |
| --- | --- |
| 평균 응답 시간 | 281.76ms |
| 중앙값 응답 시간 | 275.96ms |
| 최소 응답 시간 | 233.35ms |
| 최대 응답 시간 | 510.74ms |
| 90퍼센타일 응답 시간 | 306.70ms |
| 95퍼센타일 응답 시간 | 318.20ms |
| 에러 발생 건수 | 없음 |
| 가상 사용자 수 | 1명 |
| 초당 요청 수 | 1 RPS |
- 에러는 발생하지 않았으나, 응답 시간이 약 14배 이상 증가
- 응답 시간 변화는 지속적으로 유지되며, 일시적 스파이크가 아닌 구조적 병목 가능성 존재

## 2. 장애 전파

| 시각 | 내용 |
| --- | --- |
| **22:35** | 🔔 **Slack 알림 수신**- k6 Cloud에서 Slack Webhook을 통해 응답 지연 경고 알림 도착- 내용: "`http_req_duration` 평균 281ms로 급증 (정상: 20ms)" |
| **22:36** | 📢 **응답 지연 확인 및 장애 전파**- Grafana 및 k6 Cloud 대시보드에서 응답 시간 급증 확인- Slack `#incident-hotline` 채널에 장애 상황 공유- 메시지 예: "`[장애 전파] 응답 지연 발생 - 평균 281ms, 정상 20ms 대비 급증`" |
| **22:37** | 🛠 **시스템 로그 수집 시작**- Winston 애플리케이션 로그 수집- APM 로그 추적 시작- 서비스 담당자 `@backend-oncall` 호출 |

## 3. 조치

| 시간 | 조치 내용 |
| --- | --- |
| 22:35 | Slack 알림 수신 |
| 22:36 | Grafana 및 k6 Cloud를 통해 응답 시간 급증 확인 및 장애 전파 |
| 22:37 | 시스템 로그 수집 (Winston, APM 로그) 시작 |
| 22:40 | DB 쿼리 슬로우 로그 조회, 인덱스 상태 확인 |
| 23:00 | 시스템 리소스 모니터링 결과 CPU 사용률 95% 탐지 |
| 23:10 | API 서버 1대 재기동 |

## 4. 원인 파악

- 본 장애는 사용자 수와 요청량이 적은 상황(1 VU, 1 RPS)에서도 API 응답 시간이 평균 20ms에서 281ms 이상으로 급격히 상승한 현상에서 시작되었습니다.
- 장애 발생 당시 error 비율은 없었으며, 모든 응답은 HTTP 200으로 처리됨.
- 서버 내부 처리 병목 또는 외부 의존성(DB, 캐시 등)의 지연 가능성이 제기됨.
- 시스템 리소스를 모니터링한 결과, 장애 발생 시점 CPU 사용률이 **95%** 이상으로 확인되었으며, 이는 자원 경합으로 인한 응답 지연을 유발했을 가능성이 높음.

## 5. 심층 분석

### 📌 시스템 지표

- **평균 응답 시간**: 281.76ms
- **최대 응답 시간**: 510.74ms
- **P95 응답 시간**: 318.20ms
- **CPU 사용률**: 95%
- **메모리 사용량**: 정상 범위 (추정)

### 📌 분석 결과

1. **성능 저하 발생 위치**
    - API 수준의 병목이 존재하였으나 에러 없이 처리는 되었음.
    - 트래픽이 적음에도 응답 시간이 비정상적으로 상승한 점으로 보아, 서버 내부 병목 가능성이 큼.
2. **로그 분석 결과**
    - Winston 애플리케이션 로그 및 APM 상에서 외부 API 호출 지연이나 DB 쿼리 지연은 관측되지 않음.
    - 이는 코드 내 **동기적 블로킹 로직**, **불필요한 루프**, **CPU 집중 연산** 등의 가능성 시사.
3. **시스템 자원 분석**
    - CPU 사용률 95% 이상 지속 → 연산 집중 작업으로 인한 병목 추정
    - APM에서 특정 함수 또는 컨트롤러의 처리 시간이 평소보다 높았다는 데이터 수집됨 (예: `/product`, `/order` 등)

## 6. 후촉 조치 방안

### ✅ 단기 조치

- [x]  API 서버 재기동 및 응답 시간 정상화 확인 완료
- [ ]  동기적 연산 코드 블록 확인 (특히 리스트 순회, 정렬, 외부 연산 등)
- [ ]  과도한 요청으로 인한 시스템 과부하 방지를 위해 Rate Limiter 도입

### 🔧 중기 개선

- [ ]  APM(예: Datadog, New Relic) 전면 도입 → 코드 레벨 병목 위치 시각화
- [ ]  슬로우 쿼리 탐지 자동화 및 Slack 연동 알림 설정
- [ ]  서버 자원 경합 감시용 Grafana Alert Rule 정비 (CPU, Memory)
- [ ]  외부 시스템의 실패가 전체 시스템 장애로 확산되지 않도록 Circuit Breaker 도입

### 📈 장기 전략

- [ ]  서버 Auto Scaling 기반 구조로 점진적 이전
- [ ]  캐시(예: Redis) 사용 확대를 통한 처리 성능 향상
- [ ]  정기적인 k6 기반 성능 테스트 및 보고 체계화
- [ ]  장애 대응 Runbook 제작 및 팀원 대상 정기 교육 실시